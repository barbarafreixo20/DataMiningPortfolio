{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trabalho realizado por: Bárbara Freixo, PG49169\n",
    "\n",
    "Este código implementa o algoritmo Apriori para mineração de regras de associação. O objetivo é encontrar padrões frequentes num conjunto de transações, sendo que cada transação é uma lista de itens. O processo começa por gerar conjuntos de itens candidatos a partir dos itens únicos nas transações e, em seguida, conta-se a ocorrência de cada conjunto de itens nas transações. Conjuntos com ocorrência abaixo do suporte mínimo são descartados. Com base nos conjuntos frequentes, são gerados novos conjuntos candidatos, repetindo-se o processo até não haver mais conjuntos candidatos. Finalmente, as regras de associação são geradas com base na confiança mínima.\n",
    "\n",
    "A classe TransactionDataset representa um conjunto de transações, sendo que cada transação é uma lista de itens. O método get_unique_items é usado para obter uma lista dos itens únicos presentes em todas as transações.\n",
    "\n",
    "A classe Apriori implementa o algoritmo Apriori para mineração de regras de associação. O método count_itemsets é usado para contar a frequência de cada conjunto de itens candidato nas transações. O método prune_itemsets descarta os conjuntos de itens que têm suporte abaixo do suporte mínimo. O método generate_candidate_itemsets gera novos conjuntos de itens candidatos a partir dos conjuntos frequentes. O método apriori_algorithm é o método principal que executa o algoritmo Apriori.\n",
    "\n",
    "A função association_rules gera regras de associação a partir dos conjuntos frequentes, baseadas na confiança mínima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict, OrderedDict\n",
    "from itertools import combinations\n",
    "import unittest\n",
    "\n",
    "# Classe para representar um conjunto de transações (cada transação é uma lista de itens)\n",
    "class TransactionDataset:\n",
    "    def __init__(self, transactions):\n",
    "        self.transactions = transactions  # Lista de transações\n",
    "        self.items = self.get_unique_items()  # Lista de itens únicos\n",
    "\n",
    "    # Método para obter itens únicos em todas as transações\n",
    "    def get_unique_items(self):\n",
    "        unique_items = set()\n",
    "        for transaction in self.transactions:\n",
    "            for item in transaction:\n",
    "                unique_items.add(item)\n",
    "        return sorted(list(unique_items))\n",
    "\n",
    "# Classe para implementar o algoritmo Apriori\n",
    "class Apriori:\n",
    "    def __init__(self, transaction_dataset, min_support):\n",
    "        self.transaction_dataset = transaction_dataset  # Objeto TransactionDataset\n",
    "        self.min_support = min_support  # Suporte mínimo\n",
    "        self.itemsets = self.apriori_algorithm()  # Conjuntos de itens frequentes\n",
    "\n",
    "    # Método para contar a ocorrência de cada conjunto de itens candidato nas transações\n",
    "    def count_itemsets(self, candidate_itemsets):\n",
    "        counts = defaultdict(int)\n",
    "        for transaction in self.transaction_dataset.transactions:\n",
    "            for itemset in candidate_itemsets:\n",
    "                if set(itemset).issubset(transaction):\n",
    "                    counts[itemset] += 1\n",
    "        return counts\n",
    "\n",
    "    # Método para podar conjuntos de itens com suporte abaixo do mínimo\n",
    "    def prune_itemsets(self, counts):\n",
    "        pruned_itemsets = []\n",
    "        for itemset, count in counts.items():\n",
    "            if count / len(self.transaction_dataset.transactions) >= self.min_support:\n",
    "                pruned_itemsets.append(itemset)\n",
    "        return pruned_itemsets\n",
    "\n",
    "    # Método para gerar novos conjuntos de itens candidatos a partir dos conjuntos frequentes\n",
    "    def generate_candidate_itemsets(self, frequent_itemsets):\n",
    "        candidates = set()\n",
    "        for itemset1 in frequent_itemsets:\n",
    "            for itemset2 in frequent_itemsets:\n",
    "                union = tuple(sorted(set(itemset1).union(itemset2)))\n",
    "                if len(union) == len(itemset1) + 1:\n",
    "                    candidates.add(union)\n",
    "        return candidates\n",
    "\n",
    "    # Método principal para executar o algoritmo Apriori\n",
    "    def apriori_algorithm(self):\n",
    "        itemsets = []\n",
    "        candidate_itemsets = [tuple([item]) for item in self.transaction_dataset.items]\n",
    "        while candidate_itemsets:\n",
    "            counts = self.count_itemsets(candidate_itemsets)\n",
    "            frequent_itemsets = self.prune_itemsets(counts)\n",
    "            itemsets.extend(frequent_itemsets)\n",
    "            candidate_itemsets = self.generate_candidate_itemsets(frequent_itemsets)\n",
    "        return itemsets\n",
    "\n",
    "# Função para gerar regras de associação a partir dos conjuntos frequentes\n",
    "def association_rules(apriori, min_confidence):\n",
    "    rules = []\n",
    "    for itemset in apriori.itemsets:\n",
    "        if len(itemset) < 2:\n",
    "            continue\n",
    "        for i in range(1, len(itemset)):\n",
    "            for c in combinations(itemset, i):\n",
    "                antecedent = tuple(sorted(c))\n",
    "                consequent = tuple(sorted(set(itemset).difference(antecedent)))\n",
    "                antecedent_support = apriori.count_itemsets([antecedent])[antecedent] / len(apriori.transaction_dataset.transactions)\n",
    "                itemset_support = apriori.count_itemsets([itemset])[itemset] / len(apriori.transaction_dataset.transactions)\n",
    "                confidence = itemset_support / antecedent_support\n",
    "                if confidence >= min_confidence:\n",
    "                    rules.append((antecedent, consequent, confidence))\n",
    "    return rules"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testes e exemplos para o algoritmo implementado"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo de uso do código"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos supor que temos uma lista de transações que representam compras numa loja, onde cada transação é uma lista de itens comprados por um cliente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = [    ['leite', 'pão', 'manteiga'],\n",
    "    ['leite', 'pão'],\n",
    "    ['leite', 'pão', 'ovos'],\n",
    "    ['leite', 'pão', 'manteiga', 'ovos'],\n",
    "    ['leite', 'ovos'],\n",
    "    ['pão', 'manteiga', 'ovos'],\n",
    "    ['pão', 'manteiga']\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para executar o algoritmo Apriori, primeiro precisamos de criar um objeto TransactionDataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_dataset = TransactionDataset(transactions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, podemos criar um objeto Apriori com um suporte mínimo de 0,3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "apriori = Apriori(transaction_dataset, 0.3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, podemos gerar as regras de associação com uma confiança mínima de 0,5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antecedente:  ('leite',)\n",
      "Consequente:  ('pão',)\n",
      "Confiança:  0.7999999999999999\n",
      "\n",
      "\n",
      "Antecedente:  ('pão',)\n",
      "Consequente:  ('leite',)\n",
      "Confiança:  0.6666666666666666\n",
      "\n",
      "\n",
      "Antecedente:  ('manteiga',)\n",
      "Consequente:  ('pão',)\n",
      "Confiança:  1.0\n",
      "\n",
      "\n",
      "Antecedente:  ('pão',)\n",
      "Consequente:  ('manteiga',)\n",
      "Confiança:  0.6666666666666666\n",
      "\n",
      "\n",
      "Antecedente:  ('ovos',)\n",
      "Consequente:  ('pão',)\n",
      "Confiança:  0.75\n",
      "\n",
      "\n",
      "Antecedente:  ('pão',)\n",
      "Consequente:  ('ovos',)\n",
      "Confiança:  0.5\n",
      "\n",
      "\n",
      "Antecedente:  ('leite',)\n",
      "Consequente:  ('ovos',)\n",
      "Confiança:  0.6\n",
      "\n",
      "\n",
      "Antecedente:  ('ovos',)\n",
      "Consequente:  ('leite',)\n",
      "Confiança:  0.75\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rules = association_rules(apriori, 0.5)\n",
    "for antecedent, consequent, confidence in rules:\n",
    "    print(\"Antecedente: \", antecedent)\n",
    "    print(\"Consequente: \", consequent)\n",
    "    print(\"Confiança: \", confidence)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As regras de associação geradas representam padrões frequentes nas compras dos clientes, como por exemplo: se um cliente compra leite, ele também compra pão com uma confiança de 0.8."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestTransactionDataset(unittest.TestCase):\n",
    "    def test_get_unique_items(self):\n",
    "        transactions = [['a', 'b'], ['b', 'c'], ['a', 'c']]\n",
    "        dataset = TransactionDataset(transactions)\n",
    "        self.assertEqual(dataset.get_unique_items(), ['a', 'b', 'c'])\n",
    "\n",
    "class TestApriori(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        transactions = [['a', 'b'], ['b', 'c'], ['a', 'c']]\n",
    "        dataset = TransactionDataset(transactions)\n",
    "        self.apriori = Apriori(dataset, 0.5)\n",
    "\n",
    "    def test_count_itemsets(self):\n",
    "        candidates = [('a',), ('b',)]\n",
    "        counts = self.apriori.count_itemsets(candidates)\n",
    "        self.assertEqual(counts, {('a',): 2, ('b',): 2})\n",
    "\n",
    "    def test_prune_itemsets(self):\n",
    "        counts = {('a',): 2, ('b',): 2, ('c',): 1}\n",
    "        pruned_itemsets = self.apriori.prune_itemsets(counts)\n",
    "        self.assertEqual(pruned_itemsets, [('a',), ('b',)])\n",
    "\n",
    "    def test_generate_candidate_itemsets(self):\n",
    "        frequent_itemsets = [('a',), ('b',)]\n",
    "        candidates = self.apriori.generate_candidate_itemsets(frequent_itemsets)\n",
    "        self.assertEqual(candidates, {('a', 'b')})\n",
    "\n",
    "    def test_apriori_algorithm(self):\n",
    "       itemsets = self.apriori.apriori_algorithm()\n",
    "       self.assertEqual(itemsets, [('a',), ('b',), ('c',)])\n",
    "\n",
    "class TestAssociationRules(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        transactions = [['a', 'b'], ['b', 'c'], ['a', 'c']]\n",
    "        dataset = TransactionDataset(transactions)\n",
    "        self.apriori = Apriori(dataset, 0.5)\n",
    "\n",
    "    def test_association_rules(self):\n",
    "        rules = association_rules(self.apriori, 0.6)\n",
    "        self.assertEqual(rules, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tests():\n",
    "    test_classes = [TestTransactionDataset, TestApriori, TestAssociationRules]\n",
    "    \n",
    "    loader = unittest.TestLoader()\n",
    "    \n",
    "    suites_list = []\n",
    "    for test_class in test_classes:\n",
    "        suite = loader.loadTestsFromTestCase(test_class)\n",
    "        suites_list.append(suite)\n",
    "        \n",
    "    big_suite = unittest.TestSuite(suites_list)\n",
    "    runner = unittest.TextTestRunner(verbosity=2)\n",
    "    runner.run(big_suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_get_unique_items (__main__.TestTransactionDataset.test_get_unique_items) ... ok\n",
      "test_apriori_algorithm (__main__.TestApriori.test_apriori_algorithm) ... ok\n",
      "test_count_itemsets (__main__.TestApriori.test_count_itemsets) ... ok\n",
      "test_generate_candidate_itemsets (__main__.TestApriori.test_generate_candidate_itemsets) ... ok\n",
      "test_prune_itemsets (__main__.TestApriori.test_prune_itemsets) ... ok\n",
      "test_association_rules (__main__.TestAssociationRules.test_association_rules) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 6 tests in 0.021s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "run_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "class TestAprioriRandom(unittest.TestCase):\n",
    "    @staticmethod\n",
    "    def generate_random_transactions(num_transactions, num_items, item_pool_size):\n",
    "        item_pool = [''.join(random.choices(string.ascii_lowercase, k=3)) for _ in range(item_pool_size)]\n",
    "        transactions = []\n",
    "        for _ in range(num_transactions):\n",
    "            items = random.sample(item_pool, num_items)\n",
    "            transactions.append(items)\n",
    "        return transactions\n",
    "\n",
    "    def test_random_transactions(self):\n",
    "        random.seed(42)  # Set seed for reproducibility\n",
    "        transactions = self.generate_random_transactions(num_transactions=50, num_items=5, item_pool_size=20)\n",
    "        dataset = TransactionDataset(transactions)\n",
    "        apriori = Apriori(dataset, 0.2)\n",
    "        itemsets = apriori.apriori_algorithm()\n",
    "\n",
    "        # Test if all returned itemsets have the required support\n",
    "        for itemset in itemsets:\n",
    "            support = apriori.count_itemsets([itemset])[itemset] / len(apriori.transaction_dataset.transactions)\n",
    "            self.assertGreaterEqual(support, 0.2)\n",
    "\n",
    "        # Test if the association rules have the required confidence\n",
    "        rules = association_rules(apriori, 0.5)\n",
    "        for antecedent, consequent, confidence in rules:\n",
    "            self.assertGreaterEqual(confidence, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tests2():\n",
    "    test_classes = [\n",
    "        TestTransactionDataset,\n",
    "        TestApriori,\n",
    "        TestAssociationRules,\n",
    "        TestAprioriRandom,\n",
    "    ]\n",
    "\n",
    "    loader = unittest.TestLoader()\n",
    "\n",
    "    suites_list = []\n",
    "    for test_class in test_classes:\n",
    "        suite = loader.loadTestsFromTestCase(test_class)\n",
    "        suites_list.append(suite)\n",
    "\n",
    "    big_suite = unittest.TestSuite(suites_list)\n",
    "    runner = unittest.TextTestRunner(verbosity=2)\n",
    "    runner.run(big_suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_get_unique_items (__main__.TestTransactionDataset.test_get_unique_items) ... ok\n",
      "test_apriori_algorithm (__main__.TestApriori.test_apriori_algorithm) ... ok\n",
      "test_count_itemsets (__main__.TestApriori.test_count_itemsets) ... ok\n",
      "test_generate_candidate_itemsets (__main__.TestApriori.test_generate_candidate_itemsets) ... ok\n",
      "test_prune_itemsets (__main__.TestApriori.test_prune_itemsets) ... ok\n",
      "test_association_rules (__main__.TestAssociationRules.test_association_rules) ... ok\n",
      "test_random_transactions (__main__.TestAprioriRandom.test_random_transactions) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 7 tests in 0.035s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "run_tests2()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
