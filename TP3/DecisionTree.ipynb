{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este código implementa uma decision tree, usando diferentes critérios para escolher os melhores atributos e diferentes métodos para resolver conflitos, fazer Pre-Prunning e Pos-Prunning.\n",
    "\n",
    "Os critérios de seleção de atributos disponíveis são  Entropy, Gini Index e Gain Ratio, enquanto os métodos de resolução de conflitos são Poda, Majority voting e Class threshold.\n",
    "\n",
    "A pre-Prunning pode ser feita com base em Size, Maximum Depth ou Independence, enquanto a pos-Prunning pode ser feita com base em  Pessimistic error prunning ou Reduced error prunning.\n",
    "\n",
    "O código implementa a construção da árvore de decisão recursivamente e, em cada nó, o melhor atributo é escolhido para dividir os dados, de acordo com o critério selecionado. A impureza antes e depois da divisão é calculada e o ganho de informação é obtido a partir desses valores. Se o ganho de informação não atingir um determinado threshold, a folha é criada para esse nó. Se um dos subconjuntos resultantes da divisão for vazio, a folha é criada para esse nó."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exemplos de Aplicação**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "class CustomDecisionTreeClassifier:\n",
    "    def __init__(self, criterion='entropy', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, max_features=None,\n",
    "                 max_leaf_nodes=None, class_threshold=0.5, pre_pruning=None, post_pruning=None):\n",
    "        self.criterion = criterion\n",
    "        self.splitter = splitter\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features = max_features\n",
    "        self.max_leaf_nodes = max_leaf_nodes\n",
    "        self.class_threshold = class_threshold\n",
    "        self.pre_pruning = pre_pruning\n",
    "        self.post_pruning = post_pruning\n",
    "        self.tree = None\n",
    "\n",
    "    def _calculate_entropy(self, y):\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        probabilities = counts / counts.sum()\n",
    "        entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "        return entropy\n",
    "\n",
    "    def _calculate_gini(self, y):\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        probabilities = counts / counts.sum()\n",
    "        gini = 1 - np.sum(probabilities**2)\n",
    "        return gini\n",
    "\n",
    "    def _gain_ratio(self, gain, y, y_left, y_right):\n",
    "        split_info = -((len(y_left) / len(y)) * np.log2(len(y_left) / len(y)) + (len(y_right) / len(y)) * np.log2(len(y_right) / len(y)))\n",
    "        gain_ratio = gain / split_info\n",
    "        return gain_ratio\n",
    "    def _best_split(self, X, y):\n",
    "        best_value = 0\n",
    "        best_feature_idx = -1\n",
    "        best_threshold = None\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        if self.criterion == 'entropy':\n",
    "            impurity = self._calculate_entropy(y)\n",
    "        elif self.criterion == 'gini':\n",
    "            impurity = self._calculate_gini(y)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid criterion '{self.criterion}', use 'entropy' or 'gini'\")\n",
    "\n",
    "        for feature_idx in range(n_features):\n",
    "            feature_values = X[:, feature_idx]\n",
    "            for threshold in np.unique(feature_values):\n",
    "                mask = feature_values < threshold\n",
    "                y_left = y[mask]\n",
    "                y_right = y[~mask]\n",
    "\n",
    "                if len(y_left) < self.min_samples_split or len(y_right) < self.min_samples_split:\n",
    "                    continue\n",
    "\n",
    "                left_impurity = self._calculate_entropy(y_left) if self.criterion == 'entropy' else self._calculate_gini(y_left)\n",
    "                right_impurity = self._calculate_entropy(y_right) if self.criterion == 'entropy' else self._calculate_gini(y_right)\n",
    "                weighted_impurity = (len(y_left) * left_impurity + len(y_right) * right_impurity) / len(y)\n",
    "\n",
    "                gain = impurity - weighted_impurity\n",
    "\n",
    "                if self.splitter == 'gain_ratio':\n",
    "                    value = self._gain_ratio(gain, y, y_left, y_right)\n",
    "                else:\n",
    "                    value = gain\n",
    "\n",
    "                if value > best_value:\n",
    "                    best_value = value\n",
    "                    best_feature_idx = feature_idx\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        return best_feature_idx, best_threshold\n",
    "\n",
    "    def _build_tree(self, X, y, depth, n_nodes):\n",
    "        if depth == self.max_depth or len(np.unique(y)) == 1 or len(y) < self.min_samples_split or n_nodes == self.max_leaf_nodes:\n",
    "            return {'label': np.argmax(np.bincount(y))}\n",
    "\n",
    "        feature_idx, threshold = self._best_split(X, y)\n",
    "        \n",
    "        if threshold is None:\n",
    "            return {'label': np.argmax(np.bincount(y))}\n",
    "        \n",
    "        if self.pre_pruning == 'independence':\n",
    "            p_value = self._chi_squared_test(X, y, feature_idx, threshold)\n",
    "            if p_value > self.class_threshold:\n",
    "                return {'label': np.argmax(np.bincount(y))}\n",
    "\n",
    "        mask = X[:, feature_idx] < threshold\n",
    "        left = self._build_tree(X[mask], y[mask], depth + 1, n_nodes + 1)\n",
    "        right = self._build_tree(X[~mask], y[~mask], depth + 1, n_nodes + 1)\n",
    "\n",
    "        return {'feature_idx': feature_idx, 'threshold': threshold, 'left': left, 'right': right}\n",
    "\n",
    "    def _majority_voting_with_threshold(self, y):\n",
    "        class_counts = np.bincount(y)\n",
    "        max_count = np.max(class_counts)\n",
    "        majority_class = np.argmax(class_counts)\n",
    "        return majority_class if max_count / len(y) > self.class_threshold else None\n",
    "\n",
    "    # Pré-poda\n",
    "    # (Incluir o método _chi_squared_test já fornecido anteriormente)\n",
    "    def _chi_squared_test(self, X, y, feature_idx, threshold):\n",
    "        contingency_table = np.zeros((2, len(np.unique(y))))\n",
    "        mask = X[:, feature_idx] < threshold\n",
    "        for i, class_label in enumerate(np.unique(y)):\n",
    "            contingency_table[0, i] = np.sum(y[mask] == class_label)\n",
    "            contingency_table[1, i] = np.sum(y[~mask] == class_label)\n",
    "        \n",
    "        chi2, p_value, _, _ = chi2_contingency(contingency_table)\n",
    "        return p_value\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._build_tree(X, y, depth=0, n_nodes=0)\n",
    "\n",
    "    def _predict_sample(self, x, node):\n",
    "        if 'label' in node:\n",
    "            return node['label']\n",
    "\n",
    "        if x[node['feature_idx']] < node['threshold']:\n",
    "            return self._predict_sample(x, node['left'])\n",
    "        else:\n",
    "            return self._predict_sample(x, node['right'])\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = [self._predict_sample(x, self.tree) for x in X]\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def _reduced_error_pruning(self, node, X, y):\n",
    "        if 'label' in node:\n",
    "            return node\n",
    "\n",
    "        feature_idx = node['feature_idx']\n",
    "        threshold = node['threshold']\n",
    "        mask = X[:, feature_idx] < threshold\n",
    "        X_left, y_left = X[mask], y[mask]\n",
    "        X_right, y_right = X[~mask], y[~mask]\n",
    "\n",
    "        node['left'] = self._reduced_error_pruning(node['left'], X_left, y_left)\n",
    "        node['right'] = self._reduced_error_pruning(node['right'], X_right, y_right)\n",
    "\n",
    "        if 'label' in node['left'] and 'label' in node['right']:\n",
    "            y_pred = self.predict(X)\n",
    "            node_label = {'label': np.argmax(np.bincount(y))}\n",
    "            self.tree = node_label\n",
    "            y_pred_pruned = self.predict(X)\n",
    "\n",
    "            if np.sum(y_pred != y) >= np.sum(y_pred_pruned != y):\n",
    "                return node_label\n",
    "\n",
    "        self.tree = node\n",
    "        return node\n",
    "\n",
    "    def _pessimistic_error_pruning(self, node, X, y, n):\n",
    "        if 'label' in node:\n",
    "            node['error'] = np.sum(y != node['label'])\n",
    "            return node\n",
    "\n",
    "        feature_idx = node['feature_idx']\n",
    "        threshold = node['threshold']\n",
    "        mask = X[:, feature_idx] < threshold\n",
    "        X_left, y_left = X[mask], y[mask]\n",
    "        X_right, y_right = X[~mask], y[~mask]\n",
    "\n",
    "        node['left'] = self._pessimistic_error_pruning(node['left'], X_left, y_left, n)\n",
    "        node['right'] = self._pessimistic_error_pruning(node['right'], X_right, y_right, n)\n",
    "\n",
    "        node_error = node['left']['error'] + node['right']['error']\n",
    "        node['error'] = node_error\n",
    "        leaf_error = np.sum(y != np.argmax(np.bincount(y)))\n",
    "\n",
    "        if node_error + np.sqrt(node_error / n) >= leaf_error:\n",
    "            return {'label': np.argmax(np.bincount(y)), 'error': leaf_error}\n",
    "\n",
    "        return node\n",
    "\n",
    "\n",
    "    def prune(self, X, y):\n",
    "        if self.post_pruning == 'reduced_error_pruning':\n",
    "            self.tree = self._reduced_error_pruning(self.tree, X, y)\n",
    "        elif self.post_pruning == 'pessimistic_error_pruning':\n",
    "            self.tree = self._pessimistic_error_pruning(self.tree, X, y)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid post_pruning '{self.post_pruning}', use 'reduced_error_pruning' or 'pessimistic_error_pruning'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.96\n",
      "Accuracy after pruning: 0.90\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Carregar conjunto de dados\n",
    "breast_cancer = load_breast_cancer()\n",
    "X = breast_cancer.data\n",
    "y = breast_cancer.target\n",
    "\n",
    "# Divide o conjunto de dados em conjuntos de treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Instancia e ajusta o modelo de árvore de decisão personalizado\n",
    "dt = CustomDecisionTreeClassifier(criterion='entropy', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1,\n",
    "                                  pre_pruning='independence', post_pruning='reduced_error_pruning', class_threshold=0.3)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Faz previsões e calcula a acurácia\n",
    "y_pred = dt.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Aplica poda e calcula a acurácia novamente\n",
    "dt.prune(X_train, y_train)\n",
    "y_pred_pruned = dt.predict(X_test)\n",
    "accuracy_pruned = accuracy_score(y_test, y_pred_pruned)\n",
    "print(f\"Accuracy after pruning: {accuracy_pruned:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
