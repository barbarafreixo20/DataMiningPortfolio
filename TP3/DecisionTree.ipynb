{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este código implementa uma decision tree, usando diferentes critérios para escolher os melhores atributos e diferentes métodos para resolver conflitos, fazer Pre-Prunning e Pos-Prunning.\n",
    "\n",
    "Os critérios de seleção de atributos disponíveis são  Entropy, Gini Index e Gain Ratio, enquanto os métodos de resolução de conflitos são Poda, Majority voting e Class threshold.\n",
    "\n",
    "A pre-Prunning pode ser feita com base em Size, Maximum Depth ou Independence, enquanto a pos-Prunning pode ser feita com base em  Pessimistic error prunning ou Reduced error prunning.\n",
    "\n",
    "O código implementa a construção da árvore de decisão recursivamente e, em cada nó, o melhor atributo é escolhido para dividir os dados, de acordo com o critério selecionado. A impureza antes e depois da divisão é calculada e o ganho de informação é obtido a partir desses valores. Se o ganho de informação não atingir um determinado threshold, a folha é criada para esse nó. Se um dos subconjuntos resultantes da divisão for vazio, a folha é criada para esse nó."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class CustomDecisionTreeClassifier:\n",
    "    def __init__(self, criterion='entropy', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, max_features=None,\n",
    "                 max_leaf_nodes=None, class_threshold=0.5, pre_pruning=None, post_pruning=None):\n",
    "        self.criterion = criterion\n",
    "        self.splitter = splitter\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features = max_features\n",
    "        self.max_leaf_nodes = max_leaf_nodes\n",
    "        self.class_threshold = class_threshold\n",
    "        self.pre_pruning = pre_pruning\n",
    "        self.post_pruning = post_pruning\n",
    "        self.tree = None\n",
    "\n",
    "    def _calculate_entropy(self, y):\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        probabilities = counts / counts.sum()\n",
    "        entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "        return entropy\n",
    "\n",
    "    def _calculate_gini(self, y):\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        probabilities = counts / counts.sum()\n",
    "        gini = 1 - np.sum(probabilities**2)\n",
    "        return gini\n",
    "\n",
    "    def _gain_ratio(self, gain, y, y_left, y_right):\n",
    "        split_info = -((len(y_left) / len(y)) * np.log2(len(y_left) / len(y)) + (len(y_right) / len(y)) * np.log2(len(y_right) / len(y)))\n",
    "        gain_ratio = gain / split_info\n",
    "        return gain_ratio\n",
    "\n",
    "    def _best_split(self, X, y):\n",
    "        best_value = 0\n",
    "        best_feature_idx = -1\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        if self.criterion == 'entropy':\n",
    "            impurity = self._calculate_entropy(y)\n",
    "        elif self.criterion == 'gini':\n",
    "            impurity = self._calculate_gini(y)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid criterion '{self.criterion}', use 'entropy' or 'gini'\")\n",
    "\n",
    "        for feature_idx in range(n_features):\n",
    "            feature_values = X[:, feature_idx]\n",
    "            for threshold in np.unique(feature_values):\n",
    "                mask = feature_values < threshold\n",
    "                y_left = y[mask]\n",
    "                y_right = y[~mask]\n",
    "\n",
    "                if len(y_left) < self.min_samples_split or len(y_right) < self.min_samples_split:\n",
    "                    continue\n",
    "\n",
    "                left_impurity = self._calculate_entropy(y_left) if self.criterion == 'entropy' else self._calculate_gini(y_left)\n",
    "                right_impurity = self._calculate_entropy(y_right) if self.criterion == 'entropy' else self._calculate_gini(y_right)\n",
    "                weighted_impurity = (len(y_left) * left_impurity + len(y_right) * right_impurity) / len(y)\n",
    "\n",
    "                gain = impurity - weighted_impurity\n",
    "                \n",
    "                if self.splitter == 'gain_ratio':\n",
    "                    value = self._gain_ratio(gain, y, y_left, y_right)\n",
    "                else:\n",
    "                    value = gain\n",
    "\n",
    "                if value > best_value:\n",
    "                    best_value = value\n",
    "                    best_feature_idx = feature_idx\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        return best_feature_idx, best_threshold\n",
    "\n",
    "    def _build_tree(self, X, y, depth, n_nodes):\n",
    "        if depth == self.max_depth or len(np.unique(y)) == 1 or len(y) < self.min_samples_split or n_nodes == self.max_leaf_nodes:\n",
    "            return {'label': np.argmax(np.bincount(y))}\n",
    "\n",
    "        feature_idx, threshold = self._best_split(X, y)\n",
    "        mask = X[:, feature_idx] < threshold\n",
    "        left = self._build_tree(X[mask], y[mask], depth + 1, n_nodes + 1)\n",
    "        right = self._build_tree(X[~mask], y[~mask], depth + 1, n_nodes + 1)\n",
    "\n",
    "        return {'feature_idx': feature_idx, 'threshold': threshold, 'left': left, 'right': right}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._build_tree(X, y, depth=0, n_nodes=0)\n",
    "\n",
    "    def _predict_sample(self, x, node):\n",
    "        if 'label' in node:\n",
    "            return node['label']\n",
    "\n",
    "        if x[node['feature_idx']] < node['threshold']:\n",
    "            return self._predict_sample(x, node['left'])\n",
    "        else:\n",
    "            return self._predict_sample(x, node['right'])\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = [self._predict_sample(x, self.tree) for x in X]\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        predictions = self.predict(X)\n",
    "        probabilities = np.zeros((X.shape[0], len(np.unique(predictions))))\n",
    "        for i, pred in enumerate(predictions):\n",
    "            probabilities[i, pred] = 1\n",
    "        return probabilities\n",
    "\n",
    "    def _reduced_error_pruning(self, node, X, y):\n",
    "        if 'label' in node:\n",
    "            return node\n",
    "\n",
    "        feature_idx = node['feature_idx']\n",
    "        threshold = node['threshold']\n",
    "        mask = X[:, feature_idx] < threshold\n",
    "        X_left, y_left = X[mask], y[mask]\n",
    "        X_right, y_right = X[~mask], y[~mask]\n",
    "\n",
    "        node['left'] = self._reduced_error_pruning(node['left'], X_left, y_left)\n",
    "        node['right'] = self._reduced_error_pruning(node['right'], X_right, y_right)\n",
    "\n",
    "        if 'label' in node['left'] and 'label' in node['right']:\n",
    "            y_pred = self.predict(X)\n",
    "            node_label = {'label': np.argmax(np.bincount(y))}\n",
    "            self.tree = node_label\n",
    "            y_pred_pruned = self.predict(X)\n",
    "\n",
    "            if np.sum(y_pred != y) >= np.sum(y_pred_pruned != y):\n",
    "                return node_label\n",
    "\n",
    "        self.tree = node\n",
    "        return node\n",
    "\n",
    "    def prune(self, X, y):\n",
    "        if self.post_pruning == 'reduced_error_pruning':\n",
    "            self.tree = self._reduced_error_pruning(self.tree, X, y)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid post_pruning '{self.post_pruning}', use 'reduced_error_pruning'\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exemplos de Aplicação**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 0.91\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Criando e treinando a árvore de decisão\n",
    "dt = CustomDecisionTreeClassifier(criterion='entropy', splitter='best', max_depth=2, min_samples_split=10,\n",
    "                                  min_samples_leaf=1, max_features=None, max_leaf_nodes=None, class_threshold=0.5,\n",
    "                                  pre_pruning='size', post_pruning='reduced_error')\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Prevendo os rótulos das amostras de teste\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# Calculando a acurácia\n",
    "accuracy = np.mean(y_test == y_pred)\n",
    "print(f'Acurácia: {accuracy:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
