{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este código implementa uma decision tree, usando diferentes critérios para escolher os melhores atributos e diferentes métodos para resolver conflitos, fazer Pre-Prunning e Pos-Prunning.\n",
    "\n",
    "Os critérios de seleção de atributos disponíveis são  Entropy, Gini Index e Gain Ratio, enquanto os métodos de resolução de conflitos são Poda, Majority voting e Class threshold.\n",
    "\n",
    "A pre-Prunning pode ser feita com base em Size, Maximum Depth ou Independence, enquanto a pos-Prunning pode ser feita com base em  Pessimistic error prunning ou Reduced error prunning.\n",
    "\n",
    "O código implementa a construção da árvore de decisão recursivamente e, em cada nó, o melhor atributo é escolhido para dividir os dados, de acordo com o critério selecionado. A impureza antes e depois da divisão é calculada e o ganho de informação é obtido a partir desses valores. Se o ganho de informação não atingir um determinado threshold, a folha é criada para esse nó. Se um dos subconjuntos resultantes da divisão for vazio, a folha é criada para esse nó."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_impurity_decrease=0.0, ccp_alpha=0.0):\n",
    "        self.criterion = criterion\n",
    "        self.splitter = splitter\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_impurity_decrease = min_impurity_decrease\n",
    "        self.ccp_alpha = ccp_alpha\n",
    "        \n",
    "        self.tree_ = None\n",
    "        self.n_features_ = None\n",
    "        self.n_classes_ = None\n",
    "        self.classes_ = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.n_classes_ = len(self.classes_)\n",
    "        self.n_features_ = X.shape[1]\n",
    "        self.tree_ = self._build_tree(X, y, depth=0)\n",
    "        \n",
    "    def _build_tree(self, X, y, depth):\n",
    "        # Verificar se o nó atual é uma folha\n",
    "        if depth == self.max_depth or len(X) < self.min_samples_split:\n",
    "            return self._make_leaf_node(y)\n",
    "        # Verificar se todos os exemplos no nó atual são da mesma classe\n",
    "        elif np.unique(y).shape[0] == 1:\n",
    "            return self._make_leaf_node(y)\n",
    "        else:\n",
    "            # Escolher o melhor atributo para dividir os dados\n",
    "            best_split = self._get_best_split(X, y)\n",
    "            # Verificar se o ganho de informação é maior que o threshold de independência\n",
    "            if best_split['ig'] < self.min_impurity_decrease:\n",
    "                return self._make_leaf_node(y)\n",
    "            # Dividir os dados de acordo com o melhor atributo\n",
    "            left_idxs = X[:, best_split['feature']] <= best_split['threshold']\n",
    "            right_idxs = X[:, best_split['feature']] > best_split['threshold']\n",
    "            # Verificar se algum dos subconjuntos é vazio\n",
    "            if len(y[left_idxs]) == 0 or len(y[right_idxs]) == 0:\n",
    "                return self._make_leaf_node(y)\n",
    "            # Construir os sub-nós recursivamente\n",
    "            left_child = self._build_tree(X[left_idxs], y[left_idxs], depth+1)\n",
    "            right_child = self._build_tree(X[right_idxs], y[right_idxs], depth+1)\n",
    "            # Criar um nó de decisão com o melhor atributo e os sub-nós\n",
    "            node = {'feature': best_split['feature'], 'threshold': best_split['threshold'],\n",
    "                    'left': left_child, 'right': right_child}\n",
    "            return node\n",
    "    \n",
    "    def _make_leaf_node(self, y):\n",
    "        class_counts = np.bincount(y, minlength=self.n_classes_)\n",
    "        class_probs = class_counts / class_counts.sum()\n",
    "        return {'class_probs': class_probs}\n",
    "    \n",
    "    def _get_best_split(self, X, y):\n",
    "        best_split = None\n",
    "        best_ig = 0\n",
    "        impurity_func = self._gini if self.criterion == 'gini' else self._entropy\n",
    "        \n",
    "        for i in range(self.n_features_):\n",
    "            thresholds = np.unique(X[:, i])\n",
    "            for t in thresholds:\n",
    "                left_idxs = X[:, i] <= t\n",
    "                right_idxs = X[:, i] > t\n",
    "                \n",
    "                if self.splitter == 'random':\n",
    "                    # Selecionar aleatoriamente um subconjunto de atributos\n",
    "                    random_idxs = np.random.choice(self.n_features_, size=int(np.sqrt(self.n_features_)), replace=False)\n",
    "                    if i not in random_idxs:\n",
    "                        continue\n",
    "                \n",
    "                # Calcular a impureza antes da divisão\n",
    "                impurity_before = impurity_func(y)\n",
    "                # Calcular a impureza depois da divisão\n",
    "                impurity_left = impurity_func(y[left_idxs])\n",
    "                impurity_right = impurity_func(y[right_idxs])\n",
    "                impurity_after = (len(y[left_idxs]) / len(y)) * impurity_left + (len(y[right_idxs]) / len(y)) * impurity_right\n",
    "                # Calcular o ganho de informação\n",
    "                ig = impurity_before - impurity_after\n",
    "                # Calcular o ganho de informação normalizado pelo split information\n",
    "                if self.splitter == 'best':\n",
    "                    split_info = self._entropy(np.array([len(y[left_idxs]), len(y[right_idxs])]))\n",
    "                    ig_ratio = ig / split_info if split_info != 0 else 0\n",
    "                else:\n",
    "                    ig_ratio = ig\n",
    "                \n",
    "                # Verificar se o ganho de informação é o melhor até agora\n",
    "                if ig_ratio > best_ig:\n",
    "                    best_ig = ig_ratio\n",
    "                    best_split = {'feature': i, 'threshold': t, 'ig': ig}\n",
    "        \n",
    "        return best_split\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        probs = np.bincount(y, minlength=self.n_classes_) / len(y)\n",
    "        return -np.sum(probs * np.log2(probs + 1e-6))\n",
    "\n",
    "    def _gini(self, y):\n",
    "        probs = np.bincount(y, minlength=self.n_classes_) / len(y)\n",
    "        return 1 - np.sum(probs ** 2)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = np.zeros(X.shape[0], dtype=int)\n",
    "        \n",
    "        for i, x in enumerate(X):\n",
    "            node = self.tree_\n",
    "            while 'class_probs' not in node:\n",
    "                if x[node['feature']] <= node['threshold']:\n",
    "                    node = node['left']\n",
    "                else:\n",
    "                    node = node['right']\n",
    "            y_pred[i] = np.argmax(node['class_probs'])\n",
    "        \n",
    "        return y_pred\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exemplos de Aplicação**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "tree = DecisionTree(criterion='gini', max_depth=5, min_samples_split=10, min_samples_leaf=5)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "y_pred = tree.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "tree = DecisionTree(criterion='gain_ratio', max_depth=10, min_samples_split=20, min_samples_leaf=10)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "y_pred = tree.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
