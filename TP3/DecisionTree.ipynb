{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trabalho realizado por: Bárbara Freixo, PG49169\n",
    "\n",
    "Esta implementação define uma class personalizada para a implementação de uma árvore de decisão chamada CustomDecisionTreeClassifier. Ela apresenta várias opções para personalizar a construção da árvore de decisão, como escolher o impurity criterion (entropy ou Gini index), o splitting method (information gain ou gain ratio) e parâmetros como maximum depth, minimum number of samples for splitting, pre-prunning e post-pruning.\n",
    "\n",
    "Principais componentes do código:\n",
    "\n",
    "* Inicializador (init): Define os parâmetros da árvore de decisão, como critério de impureza, método de divisão, profundidade máxima, número mínimo de amostras para divisão, entre outros.\n",
    "* Funções para calcular entropia, índice de Gini e taxa de ganho.\n",
    "* Função _best_split: Seleciona o melhor ponto de divisão para um conjunto de dados com base no critério de impureza.\n",
    "* Função _build_tree: Constrói a árvore de decisão recursivamente com base nos parâmetros definidos.\n",
    "* Funções para pré-poda (_chi_squared_test) e pós-poda (_reduced_error_pruning e _pessimistic_error_pruning) da árvore.\n",
    "* Função fit: Treina a árvore de decisão com base no conjunto de treinamento fornecido.\n",
    "* Função predict: Faz previsões para um conjunto de dados com base na árvore de decisão treinada."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Código:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "class CustomDecisionTreeClassifier:\n",
    "\n",
    "    def __init__(self, criterion='entropy', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, max_features=None,\n",
    "                 max_leaf_nodes=None, class_threshold=0.5, pre_pruning=None, post_pruning=None, min_size=None):\n",
    "        # Inicialização dos parâmetros da árvore de decisão\n",
    "        self.criterion = criterion\n",
    "        self.splitter = splitter\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features = max_features\n",
    "        self.max_leaf_nodes = max_leaf_nodes\n",
    "        self.class_threshold = class_threshold\n",
    "        self.pre_pruning = pre_pruning\n",
    "        self.post_pruning = post_pruning\n",
    "        self.min_size = min_size\n",
    "        self.tree = None\n",
    "\n",
    "    def _calculate_entropy(self, y):\n",
    "        # Calcula a entropia de um conjunto de dados y\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        probabilities = counts / counts.sum()\n",
    "        entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "        return entropy\n",
    "\n",
    "    def _calculate_gini(self, y):\n",
    "         # Calcula o índice de Gini de um conjunto de dados y\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        probabilities = counts / counts.sum()\n",
    "        gini = 1 - np.sum(probabilities**2)\n",
    "        return gini\n",
    "\n",
    "    def _gain_ratio(self, gain, y, y_left, y_right):\n",
    "        # Calcula o ganho da razão para um determinado ganho e partições de dados\n",
    "        split_info = -((len(y_left) / len(y)) * np.log2(len(y_left) / len(y)) + (len(y_right) / len(y)) * np.log2(len(y_right) / len(y)))\n",
    "        gain_ratio = gain / split_info\n",
    "        return gain_ratio\n",
    "\n",
    "    def _best_split(self, X, y):\n",
    "        # Encontra o melhor ponto de divisão para um conjunto de dados X e y\n",
    "        best_value = 0\n",
    "        best_feature_idx = -1\n",
    "        best_threshold = None\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        if self.criterion == 'entropy':\n",
    "            impurity = self._calculate_entropy(y)\n",
    "        elif self.criterion == 'gini':\n",
    "            impurity = self._calculate_gini(y)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid criterion '{self.criterion}', use 'entropy' or 'gini'\")\n",
    "\n",
    "        for feature_idx in range(n_features):\n",
    "            feature_values = X[:, feature_idx]\n",
    "            for threshold in np.unique(feature_values):\n",
    "                mask = feature_values < threshold\n",
    "                y_left = y[mask]\n",
    "                y_right = y[~mask]\n",
    "\n",
    "                if len(y_left) < self.min_samples_split or len(y_right) < self.min_samples_split:\n",
    "                    continue\n",
    "\n",
    "                left_impurity = self._calculate_entropy(y_left) if self.criterion == 'entropy' else self._calculate_gini(y_left)\n",
    "                right_impurity = self._calculate_entropy(y_right) if self.criterion == 'entropy' else self._calculate_gini(y_right)\n",
    "                weighted_impurity = (len(y_left) * left_impurity + len(y_right) * right_impurity) / len(y)\n",
    "\n",
    "                gain = impurity - weighted_impurity\n",
    "\n",
    "                if self.splitter == 'gain_ratio':\n",
    "                    value = self._gain_ratio(gain, y, y_left, y_right)\n",
    "                else:\n",
    "                    value = gain\n",
    "\n",
    "                if value > best_value:\n",
    "                    best_value = value\n",
    "                    best_feature_idx = feature_idx\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        return best_feature_idx, best_threshold\n",
    "\n",
    "    def _build_tree(self, X, y, depth, n_nodes):\n",
    "        # Constrói a árvore de decisão recursivamente\n",
    "        if depth == self.max_depth or len(np.unique(y)) == 1 or len(y) < self.min_samples_split or n_nodes == self.max_leaf_nodes or (self.min_size is not None and len(y) < self.min_size):\n",
    "            majority_class = self._majority_voting_with_threshold(y)\n",
    "            return {'label': majority_class if majority_class is not None else np.argmax(np.bincount(y))}\n",
    "        \n",
    "        feature_idx, threshold = self._best_split(X, y)\n",
    "        \n",
    "        if threshold is None:\n",
    "            return {'label': np.argmax(np.bincount(y))}\n",
    "        \n",
    "        if self.pre_pruning == 'independence':\n",
    "            p_value = self._chi_squared_test(X, y, feature_idx, threshold)\n",
    "            if p_value > self.class_threshold:\n",
    "                return {'label': np.argmax(np.bincount(y))}\n",
    "\n",
    "        mask = X[:, feature_idx] < threshold\n",
    "        left = self._build_tree(X[mask], y[mask], depth + 1, n_nodes + 1)\n",
    "        right = self._build_tree(X[~mask], y[~mask], depth + 1, n_nodes + 1)\n",
    "\n",
    "        return {'feature_idx': feature_idx, 'threshold': threshold, 'left': left, 'right': right}\n",
    "                                \n",
    "    def _majority_voting_with_threshold(self, y):\n",
    "         # Realiza votação majoritária com um limiar para decidir a classe\n",
    "        class_counts = np.bincount(y)\n",
    "        max_count = np.max(class_counts)\n",
    "        majority_class = np.argmax(class_counts)\n",
    "        return majority_class if max_count / len(y) > self.class_threshold else None\n",
    "\n",
    "    def _chi_squared_test(self, X, y, feature_idx, threshold):\n",
    "         # Realiza o teste do qui-quadrado para verificar a independência das variáveis\n",
    "        contingency_table = np.zeros((2, len(np.unique(y))))\n",
    "        mask = X[:, feature_idx] < threshold\n",
    "        for i, class_label in enumerate(np.unique(y)):\n",
    "            contingency_table[0, i] = np.sum(y[mask] == class_label)\n",
    "            contingency_table[1, i] = np.sum(y[~mask] == class_label)\n",
    "        \n",
    "        chi2, p_value, _, _ = chi2_contingency(contingency_table)\n",
    "        return p_value\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Ajusta o modelo aos dados de treinamento\n",
    "        self.tree = self._build_tree(X, y, depth=0, n_nodes=0)\n",
    "\n",
    "    def _predict_sample(self, x, node):\n",
    "         # Prevê a classe de uma única amostra\n",
    "        if 'label' in node:\n",
    "            return node['label']\n",
    "\n",
    "        if x[node['feature_idx']] < node['threshold']:\n",
    "            return self._predict_sample(x, node['left'])\n",
    "        else:\n",
    "            return self._predict_sample(x, node['right'])\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Prevê a classe para um conjunto de dados X\n",
    "        predictions = [self._predict_sample(x, self.tree) for x in X]\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def _reduced_error_pruning(self, node, X, y, parent_majority_label=None):\n",
    "      # Realiza poda por redução de erro\n",
    "      if y.size == 0 or 'label' in node:\n",
    "          return node\n",
    "\n",
    "      feature_idx = node['feature_idx']\n",
    "      threshold = node['threshold']\n",
    "      mask = X[:, feature_idx] < threshold\n",
    "      X_left, y_left = X[mask], y[mask]\n",
    "      X_right, y_right = X[~mask], y[~mask]\n",
    "\n",
    "      majority_label = np.argmax(np.bincount(y))\n",
    "      node['left'] = self._reduced_error_pruning(node['left'], X_left, y_left, majority_label)\n",
    "      node['right'] = self._reduced_error_pruning(node['right'], X_right, y_right, majority_label)\n",
    "\n",
    "      if 'label' in node['left'] and 'label' in node['right']:\n",
    "          y_pred = self.predict(X)\n",
    "          \n",
    "          if y.size == 0:\n",
    "              node_label = {'label': parent_majority_label}\n",
    "          else:\n",
    "              node_label = {'label': np.argmax(np.bincount(y))}\n",
    "          \n",
    "          self.tree = node_label\n",
    "          y_pred_pruned = self.predict(X)\n",
    "\n",
    "          if np.sum(y_pred != y) >= np.sum(y_pred_pruned != y):\n",
    "              return node_label\n",
    "\n",
    "      self.tree = node\n",
    "      return node\n",
    "\n",
    "    def _pessimistic_error_pruning(self, node, X, y, n):\n",
    "         # Realiza poda por erro pessimista\n",
    "        if 'label' in node:\n",
    "            node['error'] = np.sum(y != node['label'])\n",
    "            return node\n",
    "\n",
    "        feature_idx = node['feature_idx']\n",
    "        threshold = node['threshold']\n",
    "        mask = X[:, feature_idx] < threshold\n",
    "        X_left, y_left = X[mask], y[mask]\n",
    "        X_right, y_right = X[~mask], y[~mask]\n",
    "\n",
    "        node['left'] = self._pessimistic_error_pruning(node['left'], X_left, y_left, n)\n",
    "        node['right'] = self._pessimistic_error_pruning(node['right'], X_right, y_right, n)\n",
    "\n",
    "        node_error = node['left']['error'] + node['right']['error']\n",
    "        node['error'] = node_error\n",
    "        leaf_error = np.sum(y != np.argmax(np.bincount(y)))\n",
    "\n",
    "        if node_error + np.sqrt(node_error / n) >= leaf_error:\n",
    "            return {'label': np.argmax(np.bincount(y)), 'error': leaf_error}\n",
    "\n",
    "        return node\n",
    "\n",
    "    def prune(self, X, y):\n",
    "        # Executa a poda da árvore de decisão\n",
    "        if self.post_pruning == 'reduced_error_pruning':\n",
    "            self.tree = self._reduced_error_pruning(self.tree, X, y)\n",
    "        elif self.post_pruning == 'pessimistic_error_pruning':\n",
    "            self.tree = self._pessimistic_error_pruning(self.tree, X, y)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid post_pruning '{self.post_pruning}', use 'reduced_error_pruning' or 'pessimistic_error_pruning'\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testes e exemplos para o algoritmo implementado"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo de uso do código"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este exemplo compara a acurácia de dois classificadores de árvore de decisão de um certo conjunto de dados: um usa a classe personalizada CustomDecisionTreeClassifier implementada anteriormente e o outro usa a implementação do sklearn DecisionTreeClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomDecisionTreeClassifier Accuracy: 0.96\n",
      "Scikit-learn DecisionTreeClassifier Accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Carrega o conjunto de dados\n",
    "breast_cancer = load_breast_cancer()\n",
    "X = breast_cancer.data\n",
    "y = breast_cancer.target\n",
    "\n",
    "# Divide o conjunto de dados em conjuntos de treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Instancia e ajusta o modelo de árvore de decisão personalizado\n",
    "dt_custom = CustomDecisionTreeClassifier(criterion='entropy', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1,\n",
    "                                         pre_pruning='independence', post_pruning='reduced_error_pruning', class_threshold=0.01)\n",
    "dt_custom.fit(X_train, y_train)\n",
    "\n",
    "# Faz previsões e calcula a acurácia\n",
    "y_pred_custom = dt_custom.predict(X_test)\n",
    "accuracy_custom = accuracy_score(y_test, y_pred_custom)\n",
    "print(f\"CustomDecisionTreeClassifier Accuracy: {accuracy_custom:.2f}\")\n",
    "\n",
    "# Instancia e ajusta o modelo de árvore de decisão do algoritmo do sklearn\n",
    "dt_sklearn = DecisionTreeClassifier(criterion='entropy', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, random_state=42)\n",
    "dt_sklearn.fit(X_train, y_train)\n",
    "\n",
    "# Faz previsões e calcula a acurácia dado pelo algoritmo do sklearn\n",
    "y_pred_sklearn = dt_sklearn.predict(X_test)\n",
    "accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
    "print(f\"Scikit-learn DecisionTreeClassifier Accuracy: {accuracy_sklearn:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testes \"Unittest\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foram realizados dois testes utilizando o unittest para testar a implementação da árvore de decisão. Temos então o primeiro teste:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este código contém testes unitários para a classe CustomDecisionTreeClassifier. Ele verifica a correta implementação das funções de cálculo de entropia, índice de Gini, votação majoritária com limite e ajuste/treino e previsão do modelo. O código cria um conjunto de dados de classificação com 100 amostras e 5 features, e verifica se a acurácia do modelo é maior que 0.8 nos dados de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.083s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "class TestCustomDecisionTreeClassifier(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        np.random.seed(42)\n",
    "        self.X, self.y = make_classification(n_samples=100, n_features=5, n_classes=2)\n",
    "        self.dt = CustomDecisionTreeClassifier()\n",
    "\n",
    "    # Testa o cálculo de entropia\n",
    "    def test_calculate_entropy(self):\n",
    "        entropy = self.dt._calculate_entropy(np.array([0, 0, 1, 1]))\n",
    "        self.assertAlmostEqual(entropy, 1.0, places=5)\n",
    "\n",
    "    # Testa o cálculo do índice de Gini\n",
    "    def test_calculate_gini(self):\n",
    "        gini = self.dt._calculate_gini(np.array([0, 0, 1, 1]))\n",
    "        self.assertAlmostEqual(gini, 0.5, places=5)\n",
    "\n",
    "    # Testa a votação majoritária com limite\n",
    "    def test_majority_voting_with_threshold(self):\n",
    "        majority_class = self.dt._majority_voting_with_threshold(np.array([0, 0, 1, 1]))\n",
    "        self.assertIsNone(majority_class)\n",
    "\n",
    "    # Testa o ajuste (fit) e previsão (predict) do modelo\n",
    "    def test_fit_and_predict(self):\n",
    "        self.dt.fit(self.X, self.y)\n",
    "        predictions = self.dt.predict(self.X)\n",
    "        accuracy = np.sum(predictions == self.y) / len(self.y)\n",
    "        self.assertGreater(accuracy, 0.8)\n",
    "\n",
    "# Executa os testes unitários\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos agora o segundo teste:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este código contém testes unitários adicionais para a classe CustomDecisionTreeClassifier. Ele verifica a correta implementação das funções de cálculo de entropia, índice de Gini, votação majoritária com limite e ajuste/treino e previsão do modelo, tanto com critério de entropia quanto com critério de índice de Gini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "......\n",
      "----------------------------------------------------------------------\n",
      "Ran 6 tests in 1.485s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class TestCustomDecisionTreeClassifier(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        np.random.seed(42)\n",
    "        self.X, self.y = make_classification(n_samples=300, n_features=10, n_classes=3, n_clusters_per_class=1)\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=0.3)\n",
    "        self.dt = CustomDecisionTreeClassifier()\n",
    "\n",
    "    # Testa o cálculo de entropia\n",
    "    def test_calculate_entropy(self):\n",
    "        entropy = self.dt._calculate_entropy(np.array([0, 0, 1, 1]))\n",
    "        self.assertAlmostEqual(entropy, 1.0, places=5)\n",
    "\n",
    "    # Testa o cálculo do índice de Gini\n",
    "    def test_calculate_gini(self):\n",
    "        gini = self.dt._calculate_gini(np.array([0, 0, 1, 1]))\n",
    "        self.assertAlmostEqual(gini, 0.5, places=5)\n",
    "\n",
    "    # Testa a votação majoritária com limite\n",
    "    def test_majority_voting_with_threshold(self):\n",
    "        majority_class = self.dt._majority_voting_with_threshold(np.array([0, 0, 1, 1]))\n",
    "        self.assertIsNone(majority_class)\n",
    "\n",
    "    # Testa o ajuste (fit) e previsão (predict) do modelo com critério de entropia\n",
    "    def test_fit_and_predict(self):\n",
    "        dt = CustomDecisionTreeClassifier(criterion='entropy', max_depth=None, min_samples_split=2, min_samples_leaf=1)\n",
    "        dt.fit(self.X_train, self.y_train)\n",
    "        predictions = dt.predict(self.X_test)\n",
    "        accuracy = accuracy_score(self.y_test, predictions)\n",
    "        self.assertGreater(accuracy, 0.8)\n",
    "\n",
    "    # Testa o processo de poda (pruning) da árvore de decisão\n",
    "    def test_pruning(self):\n",
    "        dt = CustomDecisionTreeClassifier(criterion='entropy', max_depth=None, min_samples_split=2, min_samples_leaf=1, post_pruning='reduced_error_pruning')\n",
    "        dt.fit(self.X_train, self.y_train)\n",
    "        dt.prune(self.X_test, self.y_test)\n",
    "        predictions = dt.predict(self.X_test)\n",
    "        accuracy = accuracy_score(self.y_test, predictions)\n",
    "        self.assertGreater(accuracy, 0.8)\n",
    "\n",
    "    # Testa o ajuste (fit) e previsão (predict) do modelo com critério de índice de Gini\n",
    "    def test_gini_criterion(self):\n",
    "        dt = CustomDecisionTreeClassifier(criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1)\n",
    "        dt.fit(self.X_train, self.y_train)\n",
    "        predictions = dt.predict(self.X_test)\n",
    "        accuracy = accuracy_score(self.y_test, predictions)\n",
    "        self.assertGreater(accuracy, 0.8)\n",
    "\n",
    "# Executa os testes unitários\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
